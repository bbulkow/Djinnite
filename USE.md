# ðŸ“– Using Djinnite

This guide covers how to configure Djinnite, keep your model catalog up to date, and integrate the module into your own projects.

---

## âš™ï¸ Configuration

Djinnite uses a standardized configuration system. It expects two primary JSON files in a `config/` directory at your project root.

### 1. `config/ai_config.json`
This file contains your API keys and provider-specific settings.

**Setup:**
1. Create a `config/` directory in your host project.
2. Copy `djinnite/config/ai_config.example.json` to your project's `config/ai_config.json`.
3. Add your API keys and enable the providers you want to use.
4. **Validate your configuration** immediately after setup (see below).

#### Google Gemini Configuration
Djinnite supports both **Google AI Studio** and **Vertex AI** (Google Cloud).

**For Google AI Studio (Recommended for developers):**
```json
{
  "providers": {
    "gemini": {
      "api_key": "your-gemini-api-key",
      "enabled": true,
      "default_model": "gemini-2.0-flash"
    }
  }
}
```

**For Vertex AI (Google Cloud):**
```json
{
  "providers": {
    "gemini": {
      "api_key": "your-google-cloud-api-key",
      "enabled": true,
      "default_model": "gemini-1.5-pro",
      "backend": "vertexai",
      "project_id": "your-google-cloud-project-id"
    },
    "claude": {
      "api_key": "your-anthropic-api-key",
      "enabled": true,
      "default_model": "claude-3-5-sonnet-20241022"
    }
  },
  "default_provider": "gemini"
}
```

#### Validating Your Configuration
Immediately after creating your `ai_config.json`, run the validation script. This ensures your file is correctly formatted, your API keys are valid, and connectivity is established.

**If using the standalone repository:**
```bash
uv run python -m djinnite.scripts.validate_ai
```

**If integrated into another project (as a submodule or package):**
Ensure you have installed the package (e.g., `pip install -e djinnite/`), then run:
```bash
python -m djinnite.scripts.validate_ai
```

### 2. `config/model_catalog.json`
This file is automatically generated and maintained by Djinnite scripts. It stores the capabilities and cost scores for all models discovered from provider APIs.

---

## ðŸ›  Maintenance Scripts

Keep your AI environment current by running these scripts from your project root.

### Update Model Catalog
Fetch the latest models from Gemini, Claude, and OpenAI APIs:
```bash
uv run python -m djinnite.scripts.update_models
```
*   **What it does:** Discovers new models, identifies deprecated ones, and updates context window information.
*   **Safety:** Preserves your existing manual cost overrides.

### Update Model Costs
Perform AI-powered cost estimation for all models:
```bash
uv run python -m djinnite.scripts.update_model_costs
```
*   **Anchor:** All costs are relative to **Gemini 2.5 Flash** (cost_score = 1.0).
*   **AI-Powered:** Uses an LLM to analyze pricing for other providers and assign a relative cost score.
*   **Automatic:** Calculates exact scores for Gemini models based on known pricing.

---

## ðŸ— Integration Guide (For Developers)

If you are including Djinnite as a module or git submodule in your project, follow these guidelines.

### Project Structure & Configuration Discovery
Djinnite's `config_loader.py` uses a multi-stage discovery process to find your configuration files. It checks for a `config/` directory in two locations:

1.  **Current Working Directory (CWD):** Ideal for standalone projects, CLI usage, and development.
2.  **Parent of the `djinnite/` package:** Ideal for submodule and integrated usage where Djinnite is a subdirectory of your project.

#### Recommended Structure (Integrated Use)
```
your-project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ ai_config.json        # Your secrets and preferences
â”‚   â””â”€â”€ model_catalog.json    # Generated model data
â””â”€â”€ djinnite/                 # This module (as a submodule or directory)
    â””â”€â”€ ...
```

#### Standalone Structure (Direct Use)
```
Djinnite/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ ai_config.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ djinnite/                 # (Internal package directory)
â””â”€â”€ ...
```

### Programmatic Usage
Load your configuration and initialize providers using the built-in loader:

```python
from djinnite.config_loader import load_ai_config, load_model_catalog
from djinnite.ai_providers import get_provider

# 1. Load configuration and model catalog
config = load_ai_config()
catalog = load_model_catalog()

# 2. Get the right provider and model for a specific use case
# (Defined in your ai_config.json)
provider_name, model_id = config.get_model_for_use_case("coding")

# 3. Initialize the provider
provider_config = config.get_provider(provider_name)
provider = get_provider(
    provider_name, 
    provider_config.api_key, 
    model_id
)

# 4. Use the provider â€” check max_output_tokens to avoid truncation
model_info = catalog.get_model(provider_name, model_id)
max_out = model_info.max_output_tokens if model_info else None

response = provider.generate("Hello!", max_tokens=max_out)
```

### Output Token Limits

Every model has a maximum number of output tokens it can generate. This is stored in
`ModelInfo.max_output_tokens` in the model catalog. **You should always pass this value
as `max_tokens`** to avoid silent truncation or unnecessarily low defaults.

```python
from djinnite.config_loader import load_model_catalog

catalog = load_model_catalog()
model_info = catalog.get_model("claude", "claude-sonnet-4-20250514")

if model_info:
    print(f"Max output: {model_info.max_output_tokens}")  # e.g. 16384
    response = provider.generate(prompt, max_tokens=model_info.max_output_tokens)
```

The `max_output_tokens` field is populated automatically by `update_models.py`:
- **Gemini**: Read directly from the API (`output_token_limit`)
- **Claude & OpenAI**: Looked up from a known values table, with AI-powered web search estimation as a fallback

A value of `0` means the limit is unknown â€” use a conservative default (e.g., 4096).

### Error Handling Contract

Every call to `generate()` or `generate_json()` may raise specific exceptions that callers **must** handle to avoid acting on incomplete or invalid data. These exceptions map directly to the HTTP-level failures from the underlying provider APIs.

#### Exception Hierarchy

All exceptions inherit from `AIProviderError` (which inherits from `Exception`):

```
AIProviderError                  # Base class â€” catches everything
â”œâ”€â”€ AIOutputTruncatedError       # Output hit max token limit (HTTP 200, partial content)
â”œâ”€â”€ AIContextLengthError         # Input too long for model (HTTP 400)
â”œâ”€â”€ AIRateLimitError             # Rate limit / quota exceeded (HTTP 429)
â”œâ”€â”€ AIAuthenticationError        # Invalid API key (HTTP 401)
â”œâ”€â”€ AIModelNotFoundError         # Model doesn't exist (HTTP 404)
â””â”€â”€ DjinniteModalityError        # Unsupported modality (client-side, no HTTP call)
```

#### The Two Critical Failure Modes

There are two failure modes that **will cause data corruption** if not handled:

| Failure | HTTP Status | What Happens | How Djinnite Signals It |
|---|---|---|---|
| **Output Truncated** | **200 OK** âœ… | The API returns a *partial* response because the model hit its max output token limit. The content is incomplete but looks valid. | Raises `AIOutputTruncatedError` with `e.partial_response` containing the incomplete `AIResponse` |
| **Context Too Long** | **400 Bad Request** âŒ | The API rejects the request because the input prompt exceeds the model's context window. No content is generated. | Raises `AIContextLengthError` |

**âš ï¸ The output truncation case is especially dangerous** because the underlying API returns HTTP 200 â€” it looks like a success. The provider SDKs do *not* raise an exception. Without Djinnite's check, your code would silently receive partial JSON, partial code, or partial analysis and try to act on it.

#### Provider-Specific Detection

Djinnite normalizes these provider-specific signals into the unified exception hierarchy:

| Feature | OpenAI (GPT-4o/o1) | Anthropic (Claude 3.5/3.7) | Gemini (Pro/Flash) |
|---|---|---|---|
| **Output Limit Param** | `max_tokens` (legacy) / `max_completion_tokens` (o1+) | `max_tokens` (strictly required) | `maxOutputTokens` |
| **Context Error** | 400, `code: context_length_exceeded` | 400, `type: invalid_request_error` | 400, `INVALID_ARGUMENT` |
| **Truncation Flag** | `finish_reason: "length"` | `stop_reason: "max_tokens"` | `finishReason: "MAX_TOKENS"` |

#### Required Error Handling Pattern

```python
from djinnite import (
    get_provider,
    AIOutputTruncatedError,
    AIContextLengthError,
    AIRateLimitError,
    AIProviderError,
)

provider = get_provider("gemini", api_key, model)

try:
    response = provider.generate(prompt, max_tokens=2000)
    # If we get here, the response is complete
    process(response.content)

except AIOutputTruncatedError as e:
    # The model ran out of output tokens. The response is INCOMPLETE.
    # e.partial_response contains the truncated AIResponse:
    #   - e.partial_response.content  (partial text)
    #   - e.partial_response.usage    (token counts)
    #   - e.partial_response.truncated == True
    #   - e.partial_response.finish_reason (provider-native reason)
    log.error(f"Output truncated after {e.partial_response.output_tokens} tokens")
    # Option A: Retry with higher max_tokens
    # Option B: Raise to caller
    # Option C: Use partial content if appropriate
    raise

except AIContextLengthError as e:
    # The input prompt was too long for the model.
    # No content was generated. Shorten the prompt or use a bigger model.
    log.error(f"Prompt too long: {e}")
    raise

except AIRateLimitError:
    # Rate limited â€” implement backoff or try another provider
    time.sleep(60)
    
except AIProviderError as e:
    # Catch-all for other provider errors
    log.error(f"Provider {e.provider} failed: {e}")
    raise
```

#### AIResponse Fields

On a successful (non-truncated) response, `AIResponse` includes:

```python
response.content        # Complete generated text
response.truncated      # False (always False on success)
response.finish_reason  # Provider-native reason: "stop", "end_turn", "STOP"
response.usage          # {"input_tokens": N, "output_tokens": N}
response.model          # Model ID used
response.provider       # Provider name
```

On a truncated response (available via `e.partial_response`):

```python
e.partial_response.content        # INCOMPLETE generated text
e.partial_response.truncated      # True
e.partial_response.finish_reason  # "length" (OpenAI), "max_tokens" (Claude), "MAX_TOKENS" (Gemini)
e.partial_response.usage          # Token counts (how many were actually generated)
```

---

### Using Maintenance Scripts with Custom Configs
If you are hosting your configuration outside the `djinnite/` directory (which is recommended for security), you can still use the maintenance scripts by pointing them to your project's config files. 

**Note:** Always use the `validate_ai` script first to ensure your host project's configuration is correctly formatted and authenticated.

```bash
# Validate your project's specific config file and connectivity
python -m djinnite.scripts.validate_ai --config ../config/ai_config.json

# Update your project's model catalog
python -m djinnite.scripts.update_models --config ../config/ai_config.json --catalog ../config/model_catalog.json

# Estimate costs for your project's models
python -m djinnite.scripts.update_model_costs --config ../config/ai_config.json --catalog ../config/model_catalog.json
```

### Key Principles for Integration
*   **Isolation:** Keep your `ai_config.json` out of version control. Always host it in your main project's `config/` directory, never inside the Djinnite module itself.
*   **Consistency:** Use `get_model_for_use_case` to avoid hardcoding model IDs in your logic.
*   **Updates:** Regularly run maintenance scripts against your project's config to stay current with provider APIs and pricing.

---

## ðŸ“š Appendix: API Key Guide

To use Djinnite, you'll need API keys from the providers you wish to use. Here is where you can find them:

### Google Gemini (Two Paths)

#### 1. Google AI Studio (Fastest)
1.  Go to the **[Google AI Studio](https://aistudio.google.com/app/apikey)**.
2.  Click "Create API key".
3.  Choose a project or create a new one to generate your key.
*Note: This is the default backend and has a generous free tier.*

#### 2. Vertex AI (Google Cloud Enterprise)
1.  Open the **[Google Cloud Console](https://console.cloud.google.com/)**.
2.  Select or create a project and ensure the **Vertex AI API** is enabled.
3.  Navigate to **APIs & Services > Credentials** to create an API key, or use Service Account credentials if running in a GCP environment.
4.  You will need both the **API Key** and your **Project ID**.

### Anthropic Claude
1.  Sign in to the **[Anthropic Console](https://console.anthropic.com/)**.
2.  Navigate to the "API Keys" section.
3.  Click "Create Key" to generate a new key.
*Note: You will need to add credits to your account to use Claude models.*

### OpenAI (ChatGPT)
1.  Visit the **[OpenAI Dashboard](https://platform.openai.com/api-keys)**.
2.  Click "+ Create new secret key".
3.  Ensure you copy the key immediately, as it will not be shown again.
*Note: Usage requires a paid account with available credits.*
